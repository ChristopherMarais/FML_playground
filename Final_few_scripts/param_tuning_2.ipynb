{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaf7e05-ae68-4000-935e-c765b5e930a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:30.277261Z",
     "iopub.status.busy": "2022-07-31T14:37:30.277116Z",
     "iopub.status.idle": "2022-07-31T14:37:38.993842Z",
     "shell.execute_reply": "2022-07-31T14:37:38.993442Z",
     "shell.execute_reply.started": "2022-07-31T14:37:30.277228Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x2aac0588cdf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, accuracy_score, f1_score\n",
    "cudnn.benchmark = True\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7016007e-11ea-4845-be65-b22197695002",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:38.995844Z",
     "iopub.status.busy": "2022-07-31T14:37:38.995622Z",
     "iopub.status.idle": "2022-07-31T14:37:39.001309Z",
     "shell.execute_reply": "2022-07-31T14:37:39.000954Z",
     "shell.execute_reply.started": "2022-07-31T14:37:38.995830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  NVIDIA A100-SXM4-80GB\n",
      "CPU cores:  2\n",
      "Working directory:  /blue/eee4773/gmarais/Transfer_learning/param_optim\n",
      "10:37:38\n"
     ]
    }
   ],
   "source": [
    "# use GPU when available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# Print GPU name\n",
    "print('GPU: ', torch.cuda.get_device_name(device=device))\n",
    "#number of usable CPU cores\n",
    "core_count = len(os.sched_getaffinity(0)) #os.cpu_count() for local windows machine\n",
    "print('CPU cores: ', core_count)\n",
    "# current working directory\n",
    "wd = os.getcwd()\n",
    "print('Working directory: ', wd)\n",
    "\n",
    "# start time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36b7b33-5cff-40e4-8901-d71b8de09849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:39.001822Z",
     "iopub.status.busy": "2022-07-31T14:37:39.001686Z",
     "iopub.status.idle": "2022-07-31T14:37:39.027011Z",
     "shell.execute_reply": "2022-07-31T14:37:39.026670Z",
     "shell.execute_reply.started": "2022-07-31T14:37:39.001809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, dataloaders, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    train_dict = {'train':[[],[]], 'val':[[],[]]}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                labels = labels.type(torch.LongTensor) \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            train_dict[phase][0].append(epoch_loss)\n",
    "            train_dict[phase][1].append(epoch_acc.cpu().item())\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89cc8cd8-769d-4d79-a204-78bdf59c49cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:39.027556Z",
     "iopub.status.busy": "2022-07-31T14:37:39.027428Z",
     "iopub.status.idle": "2022-07-31T14:37:39.048464Z",
     "shell.execute_reply": "2022-07-31T14:37:39.048121Z",
     "shell.execute_reply.started": "2022-07-31T14:37:39.027542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(dataloaders, epochs, gamma, step_size, learning_rate, class_num=10):\n",
    "    # load model\n",
    "    model_conv = torchvision.models.efficientnet_v2_l(weights='EfficientNet_V2_L_Weights.IMAGENET1K_V1')\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Parameters of newly constructed modules have requires_grad=True by default\n",
    "    num_ftrs = model_conv.classifier[1].in_features\n",
    "    model_conv.classifier = nn.Linear(num_ftrs, class_num)\n",
    "    model_conv = model_conv.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Observe that only parameters of final layer are being optimized\n",
    "    optimizer_conv = optim.Adam(model_conv.classifier.parameters(), lr=learning_rate)\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=step_size, gamma=gamma)\n",
    "    # train model\n",
    "    best_model, train_dict_fixed = train_model(model_conv, \n",
    "                                               criterion, \n",
    "                                               optimizer_conv, \n",
    "                                               exp_lr_scheduler, \n",
    "                                               num_epochs=epochs,\n",
    "                                               dataloaders=dataloaders)\n",
    "    return best_model, train_dict_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d5e1df-25d8-40a9-b156-509c6b5a0f04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:39.049668Z",
     "iopub.status.busy": "2022-07-31T14:37:39.049463Z",
     "iopub.status.idle": "2022-07-31T14:37:39.072993Z",
     "shell.execute_reply": "2022-07-31T14:37:39.072646Z",
     "shell.execute_reply.started": "2022-07-31T14:37:39.049653Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(best_model, dataloader, class_num=10):\n",
    "    model = torchvision.models.efficientnet_v2_l(weights='EfficientNet_V2_L_Weights.IMAGENET1K_V1')\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier = torch.nn.Linear(num_ftrs, class_num)\n",
    "    model.load_state_dict(best_model.state_dict())\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    model.train(mode=False)\n",
    "    \n",
    "    cm=0\n",
    "    cm_multi=0\n",
    "    all_labels_lst = []\n",
    "    all_preds_lst = []\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.to(int)\n",
    "            multi_labels_lst = torch.nn.functional.one_hot(labels, num_classes=class_num).cpu().tolist()\n",
    "            labels_lst = labels.cpu().tolist()\n",
    "            all_labels_lst.append(labels_lst)\n",
    "            \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            multi_preds_lst = (probabilities >= 0.9).type(torch.uint8).cpu().tolist()\n",
    "            preds_lst = torch.argmax(probabilities, axis=1).cpu().tolist()\n",
    "            all_preds_lst.append(preds_lst)\n",
    "\n",
    "            cm += confusion_matrix(y_true=labels_lst, y_pred=preds_lst, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "            cm_multi += multilabel_confusion_matrix(y_true=multi_labels_lst, y_pred=multi_preds_lst)\n",
    "            \n",
    "    all_labels_lst = sum(all_labels_lst, [])\n",
    "    all_preds_lst = sum(all_preds_lst, [])\n",
    "    print('Acc: ', accuracy_score(all_labels_lst, all_preds_lst))\n",
    "    print('Micro F1: ', f1_score(all_labels_lst, all_preds_lst, average='micro'))\n",
    "    \n",
    "    return cm, cm_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460630b-f2e2-42fc-995c-6bf7be670722",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-31T14:37:39.077552Z",
     "iopub.status.busy": "2022-07-31T14:37:39.077418Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes torch.Size([4904, 3, 300, 300]) torch.Size([1226, 3, 300, 300]) torch.Size([4904]) torch.Size([1226])\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Data shapes torch.Size([4904, 3, 300, 300]) torch.Size([1226, 3, 300, 300]) torch.Size([4904]) torch.Size([1226])\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Data shapes torch.Size([4904, 3, 300, 300]) torch.Size([1226, 3, 300, 300]) torch.Size([4904]) torch.Size([1226])\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Data shapes torch.Size([4904, 3, 300, 300]) torch.Size([1226, 3, 300, 300]) torch.Size([4904]) torch.Size([1226])\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "Already Exists\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 5 0.1\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 6.0367 Acc: 0.6531\n",
      "val Loss: 3.3807 Acc: 0.8026\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 1.1114 Acc: 0.9376\n",
      "val Loss: 0.7110 Acc: 0.9633\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.7105 Acc: 0.9647\n",
      "val Loss: 0.3960 Acc: 0.9747\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.3821 Acc: 0.9784\n",
      "val Loss: 0.3545 Acc: 0.9796\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.2629 Acc: 0.9841\n",
      "val Loss: 0.2595 Acc: 0.9821\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.1732 Acc: 0.9859\n",
      "val Loss: 0.2568 Acc: 0.9821\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.1851 Acc: 0.9876\n",
      "val Loss: 0.2540 Acc: 0.9821\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.2078 Acc: 0.9851\n",
      "val Loss: 0.2503 Acc: 0.9821\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.1686 Acc: 0.9886\n",
      "val Loss: 0.2468 Acc: 0.9829\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.1493 Acc: 0.9884\n",
      "val Loss: 0.2453 Acc: 0.9812\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.1708 Acc: 0.9886\n",
      "val Loss: 0.2415 Acc: 0.9812\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.1800 Acc: 0.9855\n",
      "val Loss: 0.2441 Acc: 0.9812\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.1722 Acc: 0.9890\n",
      "val Loss: 0.2451 Acc: 0.9812\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.1521 Acc: 0.9904\n",
      "val Loss: 0.2466 Acc: 0.9821\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.1915 Acc: 0.9886\n",
      "val Loss: 0.2505 Acc: 0.9821\n",
      "\n",
      "Training complete in 5m 26s\n",
      "Best val Acc: 0.982871\n",
      "Val\n",
      "Acc:  0.9828711256117455\n",
      "Micro F1:  0.9828711256117455\n",
      "Train\n",
      "Acc:  0.9828711256117455\n",
      "Micro F1:  0.9828711256117455\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 5 0.1\n",
      "Done at:  10:43:28\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.001\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 1.6247 Acc: 0.6130\n",
      "val Loss: 1.2264 Acc: 0.7015\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.6757 Acc: 0.9199\n",
      "val Loss: 0.3939 Acc: 0.9470\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.3646 Acc: 0.9531\n",
      "val Loss: 0.2449 Acc: 0.9682\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.2524 Acc: 0.9682\n",
      "val Loss: 0.1874 Acc: 0.9731\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.1998 Acc: 0.9725\n",
      "val Loss: 0.1567 Acc: 0.9747\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.1724 Acc: 0.9749\n",
      "val Loss: 0.1382 Acc: 0.9796\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.1505 Acc: 0.9798\n",
      "val Loss: 0.1236 Acc: 0.9796\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.1403 Acc: 0.9821\n",
      "val Loss: 0.1233 Acc: 0.9796\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.1398 Acc: 0.9816\n",
      "val Loss: 0.1233 Acc: 0.9796\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.1412 Acc: 0.9810\n",
      "val Loss: 0.1226 Acc: 0.9796\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.1406 Acc: 0.9776\n",
      "val Loss: 0.1218 Acc: 0.9796\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.1371 Acc: 0.9825\n",
      "val Loss: 0.1224 Acc: 0.9804\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.1373 Acc: 0.9810\n",
      "val Loss: 0.1219 Acc: 0.9804\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.1369 Acc: 0.9808\n",
      "val Loss: 0.1225 Acc: 0.9804\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.1368 Acc: 0.9814\n",
      "val Loss: 0.1227 Acc: 0.9796\n",
      "\n",
      "Training complete in 5m 16s\n",
      "Best val Acc: 0.980424\n",
      "Val\n",
      "Acc:  0.9804241435562806\n",
      "Micro F1:  0.9804241435562806\n",
      "Train\n",
      "Acc:  0.9804241435562806\n",
      "Micro F1:  0.9804241435562806\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.001\n",
      "Done at:  10:49:01\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.005\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 0.8937 Acc: 0.7473\n",
      "val Loss: 0.6725 Acc: 0.7977\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.1632 Acc: 0.9582\n",
      "val Loss: 0.0932 Acc: 0.9755\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.0982 Acc: 0.9759\n",
      "val Loss: 0.0676 Acc: 0.9796\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.0704 Acc: 0.9827\n",
      "val Loss: 0.0547 Acc: 0.9845\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.0649 Acc: 0.9865\n",
      "val Loss: 0.0534 Acc: 0.9853\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9861\n",
      "val Loss: 0.0498 Acc: 0.9853\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.0450 Acc: 0.9914\n",
      "val Loss: 0.0497 Acc: 0.9878\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.0427 Acc: 0.9923\n",
      "val Loss: 0.0496 Acc: 0.9886\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.0399 Acc: 0.9920\n",
      "val Loss: 0.0497 Acc: 0.9886\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.0387 Acc: 0.9939\n",
      "val Loss: 0.0498 Acc: 0.9869\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.0437 Acc: 0.9908\n",
      "val Loss: 0.0494 Acc: 0.9869\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.0397 Acc: 0.9931\n",
      "val Loss: 0.0491 Acc: 0.9869\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.0392 Acc: 0.9923\n",
      "val Loss: 0.0496 Acc: 0.9869\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.0406 Acc: 0.9918\n",
      "val Loss: 0.0495 Acc: 0.9878\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.0407 Acc: 0.9929\n",
      "val Loss: 0.0495 Acc: 0.9878\n",
      "\n",
      "Training complete in 5m 16s\n",
      "Best val Acc: 0.988581\n",
      "Val\n",
      "Acc:  0.9885807504078303\n",
      "Micro F1:  0.9885807504078303\n",
      "Train\n",
      "Acc:  0.9885807504078303\n",
      "Micro F1:  0.9885807504078303\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.005\n",
      "Done at:  10:54:35\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.01\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 1.3346 Acc: 0.5993\n",
      "val Loss: 0.6870 Acc: 0.7936\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.2077 Acc: 0.9398\n",
      "val Loss: 0.1067 Acc: 0.9682\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.1061 Acc: 0.9706\n",
      "val Loss: 0.0685 Acc: 0.9796\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9790\n",
      "val Loss: 0.0534 Acc: 0.9853\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9851\n",
      "val Loss: 0.0514 Acc: 0.9869\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.9867\n",
      "val Loss: 0.0524 Acc: 0.9861\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.0472 Acc: 0.9892\n",
      "val Loss: 0.0484 Acc: 0.9861\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.0416 Acc: 0.9904\n",
      "val Loss: 0.0484 Acc: 0.9861\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.0428 Acc: 0.9900\n",
      "val Loss: 0.0483 Acc: 0.9861\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.0377 Acc: 0.9908\n",
      "val Loss: 0.0482 Acc: 0.9861\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.0416 Acc: 0.9918\n",
      "val Loss: 0.0480 Acc: 0.9861\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.0398 Acc: 0.9929\n",
      "val Loss: 0.0480 Acc: 0.9861\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.0416 Acc: 0.9916\n",
      "val Loss: 0.0477 Acc: 0.9869\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.0365 Acc: 0.9920\n",
      "val Loss: 0.0476 Acc: 0.9878\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.0392 Acc: 0.9910\n",
      "val Loss: 0.0475 Acc: 0.9878\n",
      "\n",
      "Training complete in 5m 14s\n",
      "Best val Acc: 0.987765\n",
      "Val\n",
      "Acc:  0.9877650897226754\n",
      "Micro F1:  0.9877650897226754\n",
      "Train\n",
      "Acc:  0.9877650897226754\n",
      "Micro F1:  0.9877650897226754\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.01\n",
      "Done at:  11:00:07\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.1\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 16.9971 Acc: 0.4923\n",
      "val Loss: 7.2009 Acc: 0.7202\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 2.4137 Acc: 0.8858\n",
      "val Loss: 0.7626 Acc: 0.9617\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 1.4048 Acc: 0.9392\n",
      "val Loss: 0.6902 Acc: 0.9690\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.6914 Acc: 0.9668\n",
      "val Loss: 0.4491 Acc: 0.9812\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.5449 Acc: 0.9772\n",
      "val Loss: 0.4699 Acc: 0.9763\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.4045 Acc: 0.9782\n",
      "val Loss: 0.3516 Acc: 0.9812\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.3320 Acc: 0.9853\n",
      "val Loss: 0.3116 Acc: 0.9845\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.2527 Acc: 0.9861\n",
      "val Loss: 0.3116 Acc: 0.9845\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.2281 Acc: 0.9904\n",
      "val Loss: 0.3113 Acc: 0.9837\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.2596 Acc: 0.9863\n",
      "val Loss: 0.3100 Acc: 0.9845\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.2277 Acc: 0.9876\n",
      "val Loss: 0.3085 Acc: 0.9837\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n",
      "train Loss: 0.2455 Acc: 0.9892\n",
      "val Loss: 0.3064 Acc: 0.9845\n",
      "\n",
      "Epoch 13/15\n",
      "----------\n",
      "train Loss: 0.2522 Acc: 0.9886\n",
      "val Loss: 0.3052 Acc: 0.9845\n",
      "\n",
      "Epoch 14/15\n",
      "----------\n",
      "train Loss: 0.2348 Acc: 0.9902\n",
      "val Loss: 0.3007 Acc: 0.9845\n",
      "\n",
      "Epoch 15/15\n",
      "----------\n",
      "train Loss: 0.2393 Acc: 0.9869\n",
      "val Loss: 0.3040 Acc: 0.9845\n",
      "\n",
      "Training complete in 5m 15s\n",
      "Best val Acc: 0.984502\n",
      "Val\n",
      "Acc:  0.9845024469820555\n",
      "Micro F1:  0.9845024469820555\n",
      "Train\n",
      "Acc:  0.9845024469820555\n",
      "Micro F1:  0.9845024469820555\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.01 7 0.1\n",
      "Done at:  11:05:39\n",
      "_____________________________________________\n",
      "\n",
      "\n",
      "fold-batch_size-gamma-step_size-learning_rate 1 512 0.1 1 0.001\n",
      "Epoch 1/15\n",
      "----------\n",
      "train Loss: 1.6607 Acc: 0.6022\n",
      "val Loss: 1.2722 Acc: 0.7325\n",
      "\n",
      "Epoch 2/15\n",
      "----------\n",
      "train Loss: 0.9224 Acc: 0.9142\n",
      "val Loss: 0.8102 Acc: 0.9225\n",
      "\n",
      "Epoch 3/15\n",
      "----------\n",
      "train Loss: 0.8777 Acc: 0.9176\n",
      "val Loss: 0.8006 Acc: 0.9290\n",
      "\n",
      "Epoch 4/15\n",
      "----------\n",
      "train Loss: 0.8715 Acc: 0.9192\n",
      "val Loss: 0.8131 Acc: 0.9282\n",
      "\n",
      "Epoch 5/15\n",
      "----------\n",
      "train Loss: 0.8681 Acc: 0.9219\n",
      "val Loss: 0.8192 Acc: 0.9282\n",
      "\n",
      "Epoch 6/15\n",
      "----------\n",
      "train Loss: 0.8692 Acc: 0.9186\n",
      "val Loss: 0.8178 Acc: 0.9290\n",
      "\n",
      "Epoch 7/15\n",
      "----------\n",
      "train Loss: 0.8718 Acc: 0.9170\n",
      "val Loss: 0.8187 Acc: 0.9299\n",
      "\n",
      "Epoch 8/15\n",
      "----------\n",
      "train Loss: 0.8678 Acc: 0.9150\n",
      "val Loss: 0.8210 Acc: 0.9282\n",
      "\n",
      "Epoch 9/15\n",
      "----------\n",
      "train Loss: 0.8692 Acc: 0.9160\n",
      "val Loss: 0.8199 Acc: 0.9282\n",
      "\n",
      "Epoch 10/15\n",
      "----------\n",
      "train Loss: 0.8721 Acc: 0.9154\n",
      "val Loss: 0.8223 Acc: 0.9290\n",
      "\n",
      "Epoch 11/15\n",
      "----------\n",
      "train Loss: 0.8672 Acc: 0.9139\n",
      "val Loss: 0.8229 Acc: 0.9290\n",
      "\n",
      "Epoch 12/15\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "data = np.load(wd+'/nodup_data.npy')\n",
    "labels = np.load(wd+'/nodup_labels.npy')\n",
    "\n",
    "# reshape data\n",
    "reshaped_data = data.reshape((300,300,3,data.shape[1]))\n",
    "reshaped_data = np.moveaxis(reshaped_data, source=[0, 1, 2, 3], destination=[2, 3, 1, 0])\n",
    "labels = np.array(labels,dtype=int)\n",
    "\n",
    "# label names\n",
    "class_names = ['Stop','Yield','Red Light','Green Light','Roundabout','Right Turn Only',\n",
    "                'Do Not Enter','Crosswalk','Handicap Parking','No Parking']\n",
    "\n",
    "#######################################################################################\n",
    "# # parameter ranges\n",
    "# kfolds = 5\n",
    "# epoch_count = 15\n",
    "# batch_size_lst = [1, 32, 64]\n",
    "# gamma_lst = [0.1, 0.2, 0.3]\n",
    "# step_size_lst = [1, 9, 15]\n",
    "# lr_lst = [0.001,0.01, 0.1]\n",
    "\n",
    "# parameter ranges\n",
    "kfolds = 5\n",
    "epoch_count = 15\n",
    "batch_size_lst = [64, 128, 256, 512]\n",
    "gamma_lst = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "step_size_lst = [1, 3, 5, 7]\n",
    "lr_lst = [0.001, 0.005,0.01, 0.1]\n",
    "#######################################################################################\n",
    "\n",
    "# split data into folds\n",
    "kf = KFold(n_splits=kfolds, random_state=42, shuffle=True)\n",
    "fold = 0\n",
    "for train_index, test_index in kf.split(reshaped_data):\n",
    "    x_train, x_test = reshaped_data[train_index], reshaped_data[test_index]\n",
    "    t_train, t_test = labels[train_index], labels[test_index]\n",
    "    x_train_te = torch.Tensor(x_train)\n",
    "    x_val_te = torch.Tensor(x_test)\n",
    "    t_train_te = torch.Tensor(t_train)\n",
    "    t_val_te = torch.Tensor(t_test)\n",
    "    fold += 1\n",
    "    \n",
    "    for batch_size in batch_size_lst:\n",
    "        # turn tensors into data loaders\n",
    "        image_datasets = {'train': TensorDataset(x_train_te,t_train_te), 'val': TensorDataset(x_val_te,t_val_te)}\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], \n",
    "                                                      batch_size=batch_size, \n",
    "                                                      shuffle=True, \n",
    "                                                      num_workers=core_count) for x in ['train', 'val']}\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "        print('Data shapes',x_train_te.shape, x_val_te.shape, t_train_te.shape, t_val_te.shape)\n",
    "        \n",
    "        for gamma in gamma_lst:\n",
    "            for step_size in step_size_lst:\n",
    "                for learning_rate in lr_lst:\n",
    "                    file_name = 'multi_'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'_cm_train.npy'\n",
    "                    if file_name in os.listdir():\n",
    "                        print('Already Exists')\n",
    "                    else:\n",
    "                        print('fold-batch_size-gamma-step_size-learning_rate', fold, batch_size, gamma, step_size, learning_rate)\n",
    "                        best_model, train_dict_fixed = train(dataloaders=dataloaders, \n",
    "                                                             epochs=epoch_count, \n",
    "                                                             gamma=gamma, \n",
    "                                                             step_size=step_size, \n",
    "                                                             learning_rate=learning_rate, \n",
    "                                                             class_num=10)\n",
    "\n",
    "                        # save training epoch data to disk\n",
    "                        df = pd.DataFrame()\n",
    "                        df['train_loss'] = train_dict_fixed['train'][0]\n",
    "                        df['val_loss'] = train_dict_fixed['val'][0]\n",
    "                        df['train_acc'] = train_dict_fixed['train'][1]\n",
    "                        df['val_acc'] = train_dict_fixed['val'][1]\n",
    "                        df.to_csv(wd+'/train_'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'.csv')\n",
    "\n",
    "                        print('Val')\n",
    "                        cm, cm_multi = test(best_model=best_model, dataloader=dataloaders['val'])\n",
    "                        cm_df = pd.DataFrame(cm)\n",
    "                        cm_df.to_csv(wd+'/'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'_cm_val.csv')\n",
    "                        np.save(wd+'/multi_'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'_cm_val', cm_multi)\n",
    "\n",
    "                        print('Train')\n",
    "                        cm, cm_multi = test(best_model=best_model, dataloader=dataloaders['train'])\n",
    "                        cm_df = pd.DataFrame(cm)\n",
    "                        cm_df.to_csv(wd+'/'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'_cm_train.csv')\n",
    "                        np.save(wd+'/multi_'+str(fold)+'_'+str(batch_size)+'_'+str(gamma)+'_'+str(step_size)+'_'+str(learning_rate)+'_cm_train', cm_multi)\n",
    "\n",
    "                        # end time \n",
    "                        t = time.localtime()\n",
    "                        current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "                        print('fold-batch_size-gamma-step_size-learning_rate', fold, batch_size, gamma, step_size, learning_rate)\n",
    "                        print('Done at: ', current_time)\n",
    "                        print('_____________________________________________\\n\\n')\n",
    "print('FINAL FINISHED')\n",
    "print('_____________________________________________\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FML_a100",
   "language": "python",
   "name": "fml_a100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
